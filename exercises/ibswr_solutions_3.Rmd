---
title: "Introduction to Bayesian Statistics with R"
subtitle: "3: Exercise solutions"
author: "Jack Kuipers"
date: "5 May 2025"
output:
  pdf_document:
    fig_width: 6
    fig_height: 3
    latex_engine: xelatex
    includes:
      in_header: ./logos/header.tex
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, include = TRUE, fig.align = "center")
knitr::opts_knit$set(global.par = TRUE)
```

First we load the tidyverse and set a seed.

```{r, message=FALSE, warning=FALSE}
library(tidyverse); set.seed(42)
```


## Exercise 3.1 - MCMC

*For MCMC we can walk randomly and accept according the the MH ratio to eventually sample proportionally to any target distribution $p(x)$. For example, a Student-$t$ distribution with $\nu = 5$.*

*Examine the output MCMC chain for different lengths. How many samples would we need to get close to the Student-$t$ distribution?*

*Use the samples to estimate (see description in Bonus Exercise 3.2)*

$$\int \cos(t) f_5(t) \mathrm{d} t \, , \qquad f_{\nu}(t) = \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\pi\nu}\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{t^2}{\nu}\right)^{-\frac{\nu+1}{2} }$$
*where $f_{\nu}(t)$ is the probability density of a Student's $t$-distribution with $\nu$ degrees of freedom.*

```{r, echo = FALSE}
# simple MCMC function
# n_its is the number of iterations
# start_x the initial position
# rw_sd is the sd of the Gaussian random walk
basicMCMC <- function(n_its = 1e3, start_x = 0, rw_sd = 1, ...) {
  xs <- rep(NA, n_its) # to store all the sampled values
  x <- start_x # starting point
  xs[1] <- x # first value
  p_x <- target_density(x, ...) # probability density at current value of x
  for (ii in 2:n_its) { # MCMC iterations
    x_prop <- x + rnorm(1, mean = 0, sd = rw_sd) # Gaussian random walk to propose next x
    p_x_prop <- target_density(x_prop, ...) # probability density at proposed x
    if (runif(1) < p_x_prop/p_x) { # MH acceptance probability
      x <- x_prop # accept move
      p_x <- p_x_prop # update density
    }
    xs[ii] <- x # store current position, even when move rejected
  }
  return(xs)
}
target_density <- function(x, nu) {
  dt(x, nu) # student-t density
}
```

First we run a short chain with the default length of 1000 iterations:

```{r}
short_chain <- basicMCMC(nu = 5)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
source("./ibswr_exercise_files/colordefs.R")
xs <- 5*c(-1000:1000)/1000
ys <- dt(xs, df = 5)
dft <- data.frame(xs, ys)
# data from short chain
hist_t_plot <- function(df, dft) {
  pp <- ggplot(df)
  pp <- pp + geom_histogram(data = df, aes(x = t, y = ..density..), bins = 30,
          position = "identity", colour = "darkorange", fill = darkorangetrans) +
          ylab("Density") + xlim(c(-5, 5))
  pp <- pp + geom_point(data = dft, aes(xs, ys), colour = "dodgerblue", size = 2) + theme_bw(base_size = 18) 
  print(pp)
}
hist_t_plot(data.frame(t = short_chain), dft)
```

It's not so bad, but a few discrepancies. Let's try longer chains

```{r}
longer_chain <- basicMCMC(n_its = 1e4, nu = 5)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
hist_t_plot(data.frame(t = longer_chain), dft)
```

```{r}
even_longer_chain <- basicMCMC(n_its = 1e5, nu = 5)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
hist_t_plot(data.frame(t = even_longer_chain), dft)
```

and they start to look quite good.

For the integral we simply evaluate the cosine at each of our sampled values and get

```{r}
mean(cos(short_chain))
mean(cos(longer_chain))
mean(cos(even_longer_chain))
very_long_chain <- basicMCMC(n_its = 1e6, nu = 5)
mean(cos(very_long_chain))
```

This gets close to the numerical value

```{r}
integrate(function(x) cos(x)*dt(x, 5), -Inf, Inf)$value
```

## Bonus Exercise 3.2 - HMC

```{r, echo = FALSE}
# simple HMC function
# n_its is the number of iterations
# start_x the initial position
# L is the number of steps of numerical propagation
# under the Hamiltionian H = U + rho^2/2, U = -log(target_density)
# epsilon is the size of the steps
basicHMC <- function(n_its = 1e2, start_x = 0, L = 10, epsilon = 0.1, ...) {
  xs <- rep(NA, n_its) # to store all the sampled values
  x <- start_x # starting point
  xs[1] <- x # first value
  U_x <- U_fn(x, ...) # log density and gradient at current x
  for (ii in 2:n_its) { # HMC iterations
    rho <- rnorm(1) # normal sample (we could define scheme with different sd)
    x_prop <- x 
    # Leapfrog method to propagate under Hamiltonian: 
    rho_prop <- rho - epsilon/2*U_x$grad # half step for momentum
    for (j in 1:L) {
      x_prop <- x_prop + epsilon*rho_prop # position update 
      U_prop <- U_fn(x_prop, ...) # update gradient
      # update momentum, with a half step at the end
      rho_prop <- rho_prop - epsilon*U_prop$grad/(1 + (j==L))
    }
    MH_prob <- exp(U_x$U + rho^2/2 - U_prop$U - rho_prop^2/2)
    if (runif(1) < MH_prob) { # MH acceptance probability
      x <- x_prop # accept move
      U_x <- U_prop # update density
    }
    xs[ii] <- x # store current position, even when move rejected
  }
  return(xs)
}
target_density <- function(x, nu, grad = FALSE) {
  dens <- dt(x, nu) # Student-t density
  if (grad) { # return density and gradient
    grad <- -(nu + 1)/(nu + x^2)*x*dens
    return(list(dens = dens, grad = grad))
  } else { # return just the density
    return(dens)
  }
}
U_fn <- function(x, ...) {
  p_x <- target_density(x, ..., grad = TRUE)
  U <- -log(p_x$dens)
  grad <- -1/p_x$dens*p_x$grad
  return(list(U = U, grad = grad))
}
```

*Examine the output HMC chain for different lengths. How many samples do we now need to get close to the Student-$t$ distribution?*

*Do we get good estimates for the integral from before?*

First we run a short chain with the default length of 100 iterations:

```{r}
short_HMC_chain <- basicHMC(nu = 5)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
hist_t_plot(data.frame(t = short_HMC_chain), dft)
```

It's in the right range, but not looking so good so far. Of course we only have 100 samples compared to the 1000 from the short MCMC chain, but with the leapfrog propagation (of 10 internal steps) both have had a similar number of evaluations of the target density, while the HMC also needed the gradient evaluations.

Let's try longer chains

```{r}
longer_HMC_chain <- basicHMC(n_its = 1e3, nu = 5)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
hist_t_plot(data.frame(t = longer_HMC_chain), dft)
```

```{r}
even_longer_HMC_chain <- basicHMC(n_its = 1e4, nu = 5)
```

```{r, echo = FALSE, message=FALSE, warning=FALSE}
hist_t_plot(data.frame(t = even_longer_HMC_chain), dft)
```

and likewise it's starting to look good.

For the integral
```{r}
mean(cos(short_HMC_chain))
mean(cos(longer_HMC_chain))
mean(cos(even_longer_HMC_chain))
very_long_HMC_chain <- basicHMC(n_its = 1e5, nu = 5)
mean(cos(very_long_HMC_chain))
```

again we approach the numerical value `r integrate(function(x) cos(x)*dt(x, 5), -Inf, Inf)$value`.

For this simple target, we don't really see any benefit of HMC over the MCMC approach, but the ability of HMC to move more easily across the distribution lends it advantages for more complex and multi-modal distributions, especially in higher dimensions.

## Bonus Exercise 3.3 - Monte Carlo integration

*Computing expectations can be applied to any continuous function*

$$E[g(x)] = \int g(x) p(x) \mathrm{d} x$$

*so that integrals where we recognise $p(x)$ as (proportional to) a probability distribution may be estimated with Monte Carlo methods since* 

$$E[g(x)] \approx \frac{1}{M} \sum_{i=1}^{M} g(x_i) $$

*for $M$ random samples $x_i$ sampled according to $p(x)$. Use samples from a Gaussian to estimate the following three integrals:*

$$\int \vert x \vert \mathrm{e}^{-x^2} \mathrm{d} x \, , \qquad \int \sin(x) \mathrm{e}^{-x^2} \mathrm{d} x\, , \qquad \int \cos(x) \mathrm{e}^{-x^2} \mathrm{d} x$$
**Reminder**, *the Gaussian probability density has the following general form:*

$$
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \mathrm{e}^{- \frac{(x - \mu)^ 2}{ 2\sigma^2} }
$$

*Compare the estimated values to the exact values of the integrals.*

Looking at the three integrals we can identify a normal distribution. From the general form of a Gaussian in the reminder above, for our problems we thus need to multiply $\sqrt{\pi}$ and correctly match the respective mean and variance:

\begin{align}
\mathrm{e}^{-x^2} =  {\color{red}{\sqrt{\pi}}} \frac{1}{\sqrt{2 \pi {\color{red}{\frac{1}{2}}}}} \mathrm{e}^{- \frac{(x - {\color{red}{0}})^ 2}{ 2 {\color{red}{\frac{1}{2}} }} }
\end{align}

to the values $\mu = 0$ and $\sigma^2 = \frac{1}{2}$. Thus we sample $M$ particles with this parameterisation:

```{r Monte Carlo}
M <- 1e5
normal_samples <- rnorm(M, mean = 0, sd = sqrt(1/2))
```

Then we evaluate the samples with the three functions above (don't forget to multiply the constant factor $\sqrt{\pi}$) and average them. That's it.

```{r}
sqrt(pi)*mean(abs(normal_samples))
sqrt(pi)*mean(sin(normal_samples))
sqrt(pi)*mean(cos(normal_samples))
```

For the comparison to the real values, we integrate analytically and with `R`'s `integrate` function.

### Absolute

This integral can be evaluated by using the symmetry around the y-axis and the fact that $\frac{\mathrm{d}}{\mathrm{d}x}\mathrm{e}^{-x^2} = -2x\mathrm{e}^{-x^2}$:

```{r}
curve(abs(x)*exp(-x^2), from = -10, to = 10, ylab = expression(abs(x)*exp(-x^2)))
```

Integrated with `R`:

```{r}
integrate(function(x) abs(x)*exp(-x^2), -Inf, Inf)$value
```

### Sine

Here it suffices to look at the symmetry along the axis:

```{r}
curve(sin(x)*exp(-x^2), from = -10, to = 10, ylab = expression(sin(x)*exp(-x^2)))
```

Integrated with `R`:

```{r}
integrate(function(x) sin(x)*exp(-x^2), -Inf, Inf)$value
```

### Cosine

Estimating this integral is somewhat harder:

```{r}
curve(cos(x)*exp(-x^2), from = -10, to = 10, ylab = expression(cos(x)*exp(-x^2)))
```

Integrated with `R`:

```{r}
integrate(function(x) cos(x)*exp(-x^2), -Inf, Inf)$value
```

The exact value of the integral is $\sqrt{\pi} \exp(-\frac{1}{4})$: `r sqrt(pi)*exp(-1/4)`.
